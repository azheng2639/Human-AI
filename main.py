import os
import sys
import re
import pandas as pd
from io import StringIO
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from openai import OpenAI
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from typing import Optional
import json

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this to restrict allowed origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load OpenAI API key from environment variable
client = OpenAI(
    api_key=os.environ.get("OPENAI_API_KEY"),
)

# Define request and response models
class QueryRequest(BaseModel):
    prompt: str
    csv_data: str

class QueryResponse(BaseModel):
    response: str
    vega_lite_json: Optional[str]

class CodeResponse(BaseModel):
    code: str

def sanitize_input(query: str) -> str:
    """Sanitize input to the python REPL.
    Remove whitespace, backtick & python (if llm mistakes python console as terminal
    """

    # Removes `, whitespace & python from start
    query = re.sub(r"^(\s|`)*(?i:python)?\s*", "", query)
    # Removes whitespace & ` from end
    query = re.sub(r"(\s|`)*$", "", query)
    return query
    
def execute_panda_dataframe_code(code):
    """
    Execute the given python code and return the output. 
    References:
    1. https://github.com/langchain-ai/langchain-experimental/blob/main/libs/experimental/langchain_experimental/utilities/python.py
    2. https://github.com/langchain-ai/langchain-experimental/blob/main/libs/experimental/langchain_experimental/tools/python/tool.py
    """
     # Save the current standard output to restore later
    old_stdout = sys.stdout
    # Redirect standard output to a StringIO object to capture any output generated by the code execution
    sys.stdout = mystdout = StringIO()
    try:
            # Execute the provided code within the current environment
        cleaned_command = sanitize_input(code)
        exec(cleaned_command)
        
        # Restore the original standard output after code execution
        sys.stdout = old_stdout
                
                # Return any captured output from the executed code
        return mystdout.getvalue()
    except Exception as e:
        sys.stdout = old_stdout
        return repr(e)

# print msg in red, accept multiple strings like print statement
def print_red(*strings):
    print("\033[91m" + " ".join(strings) + "\033[0m")


# print msg in blue, , accept multiple strings like print statement
def print_blue(*strings):
    print("\033[94m" + " ".join(strings) + "\033[0m")

execute_panda_dataframe_code_tool = {
    "type" : "function",
    "function" : {
        "name" : "execute_panda_dataframe_code",
        "description" : "Executes the given code.",
        "parameters" : {
            "type" : "object",
            "properties" : {
                "code" : {
                    "type" : "string",
                    "description" : "The python code to execute."
                }
            },
            "required" : ["code"],
            "additionalProperties": False
        }
    }
}

generate_code_tool = {
    "type" : "function",
    "function" : {
        "name" : "generate_code",
        "description" : "Generate python code to perform the given task using the data provided.",
        "parameters" : {
            "type" : "object",
            "properties" : {
                "task" : {
                    "type" : "string",
                    "description" : "The task to generate python code for."
                },
                "data" : {
                    "type" : "string",
                    "description" : "The data to use in the python code."
                }
            },
            "required" : ["task"],
            "additionalProperties": False
        }
    }
}

generate_vega_lite_json_tool = {
    "type" : "function",
    "function" : {
        "name" : "generate_vega_lite_json",
        "description" : "Generate a Vega-Lite JSON specification for the given data.",
        "parameters" : {
            "type" : "object",
            "properties" : {
                "data" : {
                    "type" : "string",
                    "description" : "The data to generate the Vega-Lite JSON specification for."
                },
                "prompt" : {
                    "type" : "string",
                    "description" : "The prompt to generate the Vega-Lite JSON specification based on."
                }
            },
            "required" : ["data", "prompt"],
            "additionalProperties": False
        }
    }
}

def generate_vega_lite_json(data, prompt):
    vega_lite_prompt = f"Generate a Vega-Lite JSON specification for the following data: {data} based on the prompt: {prompt}. "
        
    system_prompt = f"""You are an AI assistant designed to generate vega-lite specifications. It is imperative that it follows this general format: {{
        "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
        width: 400,
        height: 200,
        "mark": "bar",
        "data": {{
        "values": [
        {{"category":"A", "group": "x", "value":0.1}},
        {{"category":"A", "group": "y", "value":0.6}},
        {{"category":"A", "group": "z", "value":0.9}},
        {{"category":"B", "group": "x", "value":0.7}},
        {{"category":"B", "group": "y", "value":0.2}},
        {{"category":"B", "group": "z", "value":1.1}},
        {{"category":"C", "group": "x", "value":0.6}},
        {{"category":"C", "group": "y", "value":0.1}},
        {{"category":"C", "group": "z", "value":0.2}}
        ]
        }},
        "encoding": {{
        "x": {{"field": "category"}},
        "y": {{"field": "value", "type": "quantitative"}},
        "xOffset": {{"field": "group"}},
        "color": {{"field": "group"}}
        }}
    }}
    
    You must include the schema, the data, and the mark field. The encoding should include the correct fields and types. 
    It is imperative that the vega lite specification should be stored in the variable vega_lite_json. There should be no vega lite specification in the response variable. The response variable should be used to provide relevant information. If there is an issue generating the visualization, please return an empty vega-lite JSON specification.
    """

    response = client.beta.chat.completions.parse(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": vega_lite_prompt}
        ],

        response_format=QueryResponse
    )
    if ("mark" not in response.choices[0].message.parsed.vega_lite_json):
        return QueryResponse(response="There was an issue generating the visualization, please try again", vega_lite_json="")
    return QueryResponse(response=response.choices[0].message.parsed.response, vega_lite_json=response.choices[0].message.parsed.vega_lite_json)

def generate_code(task, data):
    code_prompt = f"Generate python code to perform the following task: {task} using the data {data}. Print the result using python print(result). The python code should only be used for calculations and not for graphing or creating any visuals"
    system_prompt = f"""You are a helpful AI assistant.
        Solve tasks using your coding and language skills.
        In the following cases, suggest python code for the user to execute.
            1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
            2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.
        Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
        When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.
        Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.
        If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
        When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible. You are not responsible for generating any form of visualizations. It is imperative that you do not generate code that creates visualizations or any graphs. You are not to import any visualization libraries or modules like matplotlib.
    """

    response = client.beta.chat.completions.parse(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": code_prompt}
        ],

        response_format=CodeResponse
    )
    return CodeResponse(code=response.choices[0].message.parsed.code)

tools = [execute_panda_dataframe_code_tool, generate_code_tool, generate_vega_lite_json_tool]
tool_map = {
    "execute_panda_dataframe_code": execute_panda_dataframe_code,
    "generate_code": generate_code,
    "generate_vega_lite_json": generate_vega_lite_json
}

def query(question, system_prompt, tools, tool_map, max_iterations=10):
    messages = [{"role": "system", "content": system_prompt}]
    messages.append({"role": "user", "content": question})
    vega_lite_json = ""
    i = 0
    while i < max_iterations:
        i += 1
        print("iteration:", i)
        response = client.chat.completions.create(
            model="gpt-4o-mini", temperature=0.0, messages=messages, tools=tools
        )
        if response.choices[0].message.content != None:
            print_red(response.choices[0].message.content)
        # if not function call
        if response.choices[0].message.tool_calls == None:
            break

        # if function call
        messages.append(response.choices[0].message)
        for tool_call in response.choices[0].message.tool_calls:
            print_blue("calling:", tool_call.function.name, "with", tool_call.function.arguments)
            # call the function
            arguments = json.loads(tool_call.function.arguments)
            function_to_call = tool_map[tool_call.function.name]
            output = function_to_call(**arguments)
            if tool_call.function.name == "generate_vega_lite_json":
                vega_lite_json = output.vega_lite_json
            # create a message containing the result of the function call
            result_content = json.dumps({**arguments, "result": str(output)})
            function_call_result_message = {
                "role": "tool",
                "content": result_content,
                "tool_call_id": tool_call.id,
            }
            print_blue("action result:", result_content)

            messages.append(function_call_result_message)
        
        if i == max_iterations and response.choices[0].message.tool_calls != None:
            print_red("Max iterations reached")
            return QueryResponse(response="The tool agent could not complete the task in the given time. Please try again.", vega_lite_json="")
    return QueryResponse(response=response.choices[0].message.content, vega_lite_json=vega_lite_json)

@app.post("/query", response_model=QueryResponse)
async def query_openai(request: QueryRequest):
    try:
        print(request.csv_data)
        if json.loads(request.csv_data) == []:
            return QueryResponse(response="Please provide a valid CSV data", vega_lite_json="")
        prompt = request.prompt + request.csv_data
        system_prompt = f"""
                        You are a helpful assistant. Use the supplied tools to assist the user when needed. 
                        Determine if the user's question is relevant to the data provided. If it is not, ask 
                        the user to provide a relevant prompt. An explanation should be provided using the response variable. 
                        The vega lite specification should be stored in the vega_lite_json variable. 
                        Once code is generated, run it using the provided tools. You are not to tell the user about
                        any code generated, including whether or not it was able to be executed. All 
                        visualizations should be done with the provided tool and not through code. 
                        All vega lite specification generated should not be displayed to the user.
                        It is imperative that you do not return python code for visualizations. The user may ask you questions that don't involve the data, but you 
                        should ignore them and focus on the data and return an empty vega-lite JSON specification as well as an explanation. 
                        """
        return query(prompt, system_prompt, tools, tool_map)
    except Exception as e:
        return QueryResponse(response="An error occurred. Please try again", vega_lite_json="")